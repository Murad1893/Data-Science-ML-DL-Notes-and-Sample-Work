{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d65d0f1-001f-4ee4-9dac-484705911345",
   "metadata": {},
   "source": [
    "### Training and Validation Loops\n",
    "\n",
    "**Date:** 30/10/2021  \n",
    "**Author:** Murad Popattia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b237b-583d-4355-bb5d-18f64b8a82a9",
   "metadata": {},
   "source": [
    "Code block for explaining use of asteriks:\n",
    "\n",
    "```\n",
    ">>> mylist = [1,2,3]\n",
    ">>> foo(*mylist)\n",
    "x=1\n",
    "y=2\n",
    "z=3\n",
    "\n",
    ">>> mydict = {'x':1,'y':2,'z':3}\n",
    ">>> foo(**mydict)\n",
    "x=1\n",
    "y=2\n",
    "z=3\n",
    "\n",
    ">>> mytuple = (1, 2, 3)\n",
    ">>> foo(*mytuple)\n",
    "x=1\n",
    "y=2\n",
    "z=3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "946bb4be-8d67-4092-a7aa-60613289c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample training step\n",
    "def train_one_step(model, data, optimizer):\n",
    "    optimizer.zero_grad() # we do this before every step\n",
    "    \n",
    "    # sending data over the gpu\n",
    "    for k,v in data.items:\n",
    "        data[k] = v.to(\"cuda\")\n",
    "        \n",
    "    # can also use this if model(x,y) the argument are same as the keys in the dictionary\n",
    "    # loss = model(**data)\n",
    "    \n",
    "    loss = model(x=data[\"x\"], y = data[\"y\"])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss \n",
    "\n",
    "# sample validation step\n",
    "def valid_one_step(model, data, optimizer):\n",
    "    # sending data over the gpu\n",
    "    for k,v in data.items:\n",
    "        data[k] = v.to(\"cuda\")    \n",
    "    loss = model(x=data[\"x\"], y = data[\"y\"])\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c418e1a9-86d0-46f9-ba74-14fb8c7f679b",
   "metadata": {},
   "source": [
    "model.train() tells your model that you are training the model. So effectively layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly.\n",
    "\n",
    "You can call either model.eval() or model.train(mode=False) to tell that you are testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04876323-4c1a-49e7-99e3-ab439a32d142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, optimizer):\n",
    "    \n",
    "    # switch model to train mode\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, data in enumerate(data_loader):\n",
    "        loss = train_one_step(model, data, optimizer)\n",
    "        # in case after every epoch we want to step the scheduler we can also do\n",
    "        scheduler.step()\n",
    "        total_loss += loss\n",
    "    return total_loss\n",
    "\n",
    "def valid_one_epoch(model, data_loader, optimizer):\n",
    "    \n",
    "    # switch model to train mode\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for idx, data in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            loss = valid_one_step(model, data, optimizer)\n",
    "            # in case after every epoch we want to step the scheduler we can also do\n",
    "            total_loss += loss\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
