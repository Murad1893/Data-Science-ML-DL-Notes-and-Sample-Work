{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed0cc23e-03b4-4ad3-a85e-abdb2c04c2fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Autograd\n",
    "\n",
    "Used in the backpropogation step in the NN \n",
    "\n",
    "**Date:** 28/10/2021  \n",
    "**Author:** Murad Popattia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a65d8f4-2abd-405e-b553-81ff7c96b70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f91149b9-5aaa-4412-a2a0-ccbac29874af",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([5.], requires_grad=True)\n",
    "b = torch.tensor([5.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0daa996-a502-4972-a56d-a52e9f16331e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5.], requires_grad=True), tensor([5.], requires_grad=True))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da5d390b-5e36-450f-9121-ae3c7ee31409",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = a ** 3 - b ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "729dbf65-3f00-40d1-8bfe-9bc0a6879f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For computing gradients we would do\n",
    "# dy / da and dy / db\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5abd7024-d7cb-4fde-832e-8e85a18b044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we would compute gradient over y\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38161c1e-89e2-499e-8fd6-e8dc703da332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([75.]), tensor([-10.]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we have values for individual values done y AUTOGRAD\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c35cec-20e5-42bf-9c90-fce24d2019db",
   "metadata": {},
   "source": [
    "### Simulating a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61832c69-bcb6-42c7-9d8c-65e9cb39b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_features x n_units -> n_units:1 in this case as we have a single output\n",
    "W = torch.rand(10,1, requires_grad = True) \n",
    "\n",
    "# n_samples x n_features\n",
    "x = torch.rand(1, 10, requires_grad = True) \n",
    "\n",
    "# n_features x 1\n",
    "b = torch.rand(1, 1, requires_grad = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5bb8798a-9cd7-4da9-bcf2-0c3a5c2e2b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Wx + b\n",
    "output = x.mm(W) + b\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "867c65da-69af-4ca5-bdb4-95ceec4c099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward prop\n",
    "loss = 1 - output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "41416cfd-1e8c-422f-9b26-d1710ca62f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.4505]], grad_fn=<RsubBackward1>),\n",
       " tensor([[3.4505]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd3b98c-6568-46bd-ab1e-cd062c11e3d9",
   "metadata": {},
   "source": [
    "We see there are associated grad functions with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "44d8e72f-eac3-4db9-8ec2-9f35e6c6558a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(W.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec628ebd-cc7a-414a-a861-91cd3aad58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we backprop\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "46b04cdb-4a7c-48ae-9e2a-535aacce60c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7254],\n",
       "        [-0.6594],\n",
       "        [-0.4947],\n",
       "        [-0.2016],\n",
       "        [-0.7754],\n",
       "        [-0.0114],\n",
       "        [-0.8516],\n",
       "        [-0.6948],\n",
       "        [-0.8337],\n",
       "        [-0.1812]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking gradients\n",
    "W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1596d53a-9492-4371-b154-0705644be751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8652],\n",
       "        [0.5214],\n",
       "        [0.2669],\n",
       "        [0.7209],\n",
       "        [0.4014],\n",
       "        [0.3441],\n",
       "        [0.1958],\n",
       "        [0.6894],\n",
       "        [0.6468],\n",
       "        [0.5280]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now update the W\n",
    "lr = 0.001\n",
    "\n",
    "# we don't calculate grad here, just updation\n",
    "# this helps avoid unecessary computation\n",
    "with torch.no_grad():\n",
    "    W = W - lr * W.grad.data\n",
    "    \n",
    "W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
