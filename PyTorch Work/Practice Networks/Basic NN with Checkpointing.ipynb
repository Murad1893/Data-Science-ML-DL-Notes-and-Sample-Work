{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3182fca-0bdf-4bec-b4fc-56e9de8727a9",
   "metadata": {},
   "source": [
    "### Basic Neural Network\n",
    "\n",
    "Creating a basic neural network using the MNIST dataset\n",
    "\n",
    "**Date:** 30/10/2021  \n",
    "**Author:** Murad Popattia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83319d97-057f-48e6-ad9d-dae99dc92886",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4984a734-b7df-4a33-8943-70b539b2ebd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn # all the layers\n",
    "import torch.optim as optim # all the optimization algos\n",
    "import torch.nn.functional as F # all the activation functions\n",
    "from torch.utils.data import DataLoader # helps to create mini-batches etc.\n",
    "import torchvision.datasets as datasets # for getting all datasets\n",
    "import torchvision.transforms as transforms # for transformations on the dataset\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d5d1a3-23d8-4cad-9639-278cad509bdb",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ed4ed8a-f4c6-4198-9ae4-c538f78a2989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create FNN\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes): # 784 input size as the images are 28 * 28\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes) # output of the network\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # adding non-linearity\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953145db-5128-43f4-9f49-629b30deb412",
   "metadata": {},
   "source": [
    "Making sure the NN is properly made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86140a6f-7c64-415f-a96d-a831a7917c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "# checking the NN\n",
    "\n",
    "model = NN(784, 10)\n",
    "features = torch.rand(64,784) # no. of examples\n",
    "output = model(features)\n",
    "print(output.shape) # output should be no. of examples x num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7f8b58-87bb-4495-829d-2dcd1bec1fe6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setting the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afcef069-14d5-4a01-b412-2fea342055a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe504b19-2258-4f0b-8e3a-21c78a778409",
   "metadata": {},
   "source": [
    "### Initializing hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0949652-c3ae-4b80-b13b-24da85656297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "input_size = 28 * 28\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 5\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a545b3a-2cae-4caf-b1b3-f96eda8c04fd",
   "metadata": {},
   "source": [
    "### Loading the dataset and creating dataloader\n",
    "\n",
    "Also transforming the dataset to convert them to tensors before sending to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b90cb6-6439-4cbb-b7ef-2c5510cb15e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16315fb6-278e-4ebd-988f-fc5460114e50",
   "metadata": {},
   "source": [
    "### Initializing the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6fb365ab-4056-49c8-80bd-96a83713eb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device) # sending model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d865d33d-2971-44b9-8c25-e9b5a342b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NN                                       --                        --\n",
       "├─Linear: 1-1                            [938, 50]                 39,250\n",
       "├─Linear: 1-2                            [938, 10]                 510\n",
       "==========================================================================================\n",
       "Total params: 39,760\n",
       "Trainable params: 39,760\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 37.29\n",
       "==========================================================================================\n",
       "Input size (MB): 2.94\n",
       "Forward/backward pass size (MB): 0.45\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 3.55\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model, input_size=(len(train_loader),input_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d00e03-b965-41a6-a308-a5ef4b9ec5ea",
   "metadata": {},
   "source": [
    "### Initializing loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e91fcddf-2e31-43f6-bda8-27e2ae198721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9826218-b1a6-48a4-be38-73144113f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def save_checkpoint(state, filepath = \"./model_checkpoints/model_checkpoint.pth.tar\"):\n",
    "    print(\"==> Saving checkpoint\")\n",
    "    torch.save(state, filepath)\n",
    "    \n",
    "def load_checkpoint(filepath = \"./model_checkpoints/model_checkpoint.pth.tar\"):\n",
    "    if os.path.isfile(filepath): # check if the file exists\n",
    "        print(\"==> Loading checkpoint\")\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        \n",
    "        return model, optimizer\n",
    "        \n",
    "    else:\n",
    "        print(\"No checkpoint found. Skipping ....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8a94a-7e93-406d-b756-bd3c32a9dbc3",
   "metadata": {},
   "source": [
    "### Loading, Training and Saving checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab0ad9b3-e69c-4cb7-b1ee-1e2b222e3931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading checkpoint\n",
      "Epoch 1/5\n",
      "==> Saving checkpoint\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0652 - validation loss: 0.0978\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0598 - validation loss: 0.0922\n",
      "Epoch 3/5\n",
      "==> Saving checkpoint\n",
      "938/938 [==============================] - 7s 8ms/step - loss: 0.0528 - validation loss: 0.0932\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0484 - validation loss: 0.1013\n",
      "Epoch 5/5\n",
      "==> Saving checkpoint\n",
      "938/938 [==============================] - 8s 8ms/step - loss: 0.0446 - validation loss: 0.0956\n"
     ]
    }
   ],
   "source": [
    "if load_model:\n",
    "    load_checkpoint()\n",
    "\n",
    "for epoch in range(num_epochs):  # 1 epoch means the model has seen all the images\n",
    "    total_loss = 0\n",
    "    counter = 0 \n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    pbar = tf.keras.utils.Progbar(target=n_batches)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    if epoch % 2 == 0:\n",
    "        checkpoint = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint)\n",
    "    \n",
    "    for idx, (data, target) in enumerate(train_loader):  # enumerating to see the batch idx\n",
    "        \n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # torch.Size([64, 1, 28, 28])\n",
    "        # data.shape[0] refers to batch size\n",
    "        # flattening the output to get correct shop\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # foward\n",
    "        scores = model(data)  \n",
    "        loss = criterion(scores, target)\n",
    "        total_loss += loss\n",
    "        \n",
    "        pbar.update(idx, values=[(\"loss\",loss.item())])\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # increment for every batch\n",
    "        counter += 1\n",
    "        \n",
    "    # for every epoch calculate test loss\n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        with torch.no_grad(): # no computation for gradients\n",
    "            # get data to cuda\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            pbar.update(idx, values=[(\"validation loss\",loss.item())])\n",
    "    \n",
    "    pbar.update(n_batches, values=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b85a74-bbe5-47c2-b346-ac1eb8b6e69c",
   "metadata": {},
   "source": [
    "As we can see that after loading the training loss starte from 0.0652 instead of ~0.421 and hence this means the checkpoint was loaded succesfully"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f45aa40-665e-493c-bb8f-ad3bafb58beb",
   "metadata": {},
   "source": [
    "### Checking the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4b4ca128-2e21-4257-b1d9-4f74f7a4a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    output_gt = []\n",
    "    output_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1) # .max() return max value and indices\n",
    "            \n",
    "            output_gt.extend(y.cpu().numpy())\n",
    "            output_pred.extend(predictions.cpu().numpy())\n",
    "        \n",
    "        if (loader.dataset.train):\n",
    "            print(\"Training Metrics: \")\n",
    "        else:\n",
    "            print(\"Testing Metrics: \")\n",
    "        print(f'Precision: {precision_score(output_gt, output_pred, average=\"micro\")}')\n",
    "        print(f'Recall: {recall_score(output_gt, output_pred, average=\"micro\")}')\n",
    "        print(f'F1_score: {f1_score(output_gt, output_pred, average=\"micro\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6dd06366-8d24-42dc-a1d8-059730665eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics: \n",
      "Precision: 0.9742333333333333\n",
      "Recall: 0.9742333333333333\n",
      "F1_score: 0.9742333333333333\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "dd5962cb-fd8d-4eb4-ad4d-314bfafd2914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Metrics: \n",
      "Precision: 0.9672\n",
      "Recall: 0.9672\n",
      "F1_score: 0.9672\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f185ca-074f-48a3-b62b-a568b2b41e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
