{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30214330-26d6-495f-ae86-779807824942",
   "metadata": {},
   "source": [
    "### Tensorboard Integration with Pytorch\n",
    "\n",
    "**Date:** 15/11/2021  \n",
    "**Author:** Murad Popattia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "240a79dd-3feb-4a26-9cb9-8e0097f35e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn # all the layers\n",
    "import torch.optim as optim # all the optimization algos\n",
    "import torch.nn.functional as F # all the activation functions\n",
    "from torch.utils.data import DataLoader # helps to create mini-batches etc.\n",
    "import torchvision.datasets as datasets # for getting all datasets\n",
    "import torchvision.transforms as transforms # for transformations on the dataset\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # to print tensorboard\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48eec2ef-a9b5-4a4c-92e4-cf22f330c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating our CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, in_channels = 1, num_classes = 10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=(3,3), stride=(1,1), padding=(1,1)) # same convolution : dimensions dont change\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3,3), stride=(1,1), padding=(1,1))\n",
    "#         # 28 -> 14 -> 7\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # adding non-linearity for filters\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x) # reusing the same pooling\n",
    "        x = x.reshape(x.shape[0], -1) # keep the mini-batches and flatten the rest out\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4de089-a09b-4f4a-bcd6-757169987219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper params\n",
    "in_channels = 1\n",
    "num_classes = 10\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18add1b6-4ee5-4684-b522-273af120bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the network\n",
    "model = CNN().to(device) # sending model to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5354b9ce-5c6b-4661-84d3-10979ff1c2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]              80\n",
      "         MaxPool2d-2            [-1, 8, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 14, 14]           1,168\n",
      "         MaxPool2d-4             [-1, 16, 7, 7]               0\n",
      "            Linear-5                   [-1, 10]           7,850\n",
      "================================================================\n",
      "Total params: 9,098\n",
      "Trainable params: 9,098\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.09\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce7a899-4721-4e27-beac-1108291dc76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46fc754-37b8-491f-a145-3405700fc79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='../datasets/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='../datasets/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063d30b7-3d57-4e12-bf0e-250af2c73a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 11s 12ms/step - loss: 0.3532 - acc: : 0.8933 - validation loss: 0.1229 - validation acc: : 0.9618\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 11s 979us/step - loss: 0.1076 - acc: : 0.9673 - validation loss: 0.0887 - validation acc: : 0.9724\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 11s 11ms/step - loss: 0.0795 - acc: : 0.9755 - validation loss: 0.0708 - validation acc: : 0.9762\n",
      "Epoch 4/5\n",
      "670/938 [====================>.........] - ETA: 2s - loss: 0.0656 - acc: : 0.9802"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # 1 epoch means the model has seen all the images\n",
    "    total_loss = 0\n",
    "    counter = 0 \n",
    "    \n",
    "    n_batches = len(train_loader)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    pbar = tf.keras.utils.Progbar(target=n_batches)\n",
    "    \n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "    for idx, (data, target) in enumerate(train_loader):  # enumerating to see the batch idx\n",
    "        \n",
    "        # get data to cuda if possible\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # foward\n",
    "        scores = model(data)  \n",
    "        loss = criterion(scores, target)\n",
    "        total_loss += loss\n",
    "        \n",
    "        pbar.update(idx, values=[(\"loss\",loss.item())])\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # increment for every batch\n",
    "        counter += 1\n",
    "        \n",
    "        # using a running training accuracy\n",
    "        _, preds = scores.max(1)\n",
    "        num_correct = (preds == target).sum()\n",
    "        \n",
    "        pbar.update(idx, values=[(\"acc: \",float(num_correct)/float(data.shape[0]))])\n",
    "        \n",
    "    # for every epoch calculate test loss\n",
    "    # validation\n",
    "    model.eval()\n",
    "    \n",
    "    for idx, (data, target) in enumerate(test_loader):\n",
    "        with torch.no_grad(): # no computation for gradients\n",
    "            # get data to cuda\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            scores = model(data)\n",
    "            loss = criterion(scores, target)\n",
    "            \n",
    "            pbar.update(idx, values=[(\"validation loss\",loss.item())])\n",
    "            \n",
    "            # using a running training accuracy\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct = (preds == target).sum()\n",
    "            \n",
    "            pbar.update(idx, values=[(\"validation acc\",float(num_correct)/float(data.shape[0]))])\n",
    "    \n",
    "    pbar.update(n_batches, values=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925808c4-98f8-4960-b3b3-eca57464c244",
   "metadata": {},
   "source": [
    "### Adding batch sizes and learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42449553-d9a6-411d-a490-e597a36a0eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [8,64] # we don't change the bz for test\n",
    "learning_rates = [0.001,0.0001]\n",
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a5493be-bc48-4b36-b577-6b12bc13f606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for Batch_size = 8, Learning_rate = 0.001\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 72s 128us/step - loss: 0.1619 - acc: : 0.9503 - validation loss: 0.0636 - validation acc: 0.9786\n",
      "Running for Batch_size = 8, Learning_rate = 0.0001\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 71s 9ms/step - loss: 0.5192 - acc: : 0.8616 - validation loss: 0.2173 - validation acc: 0.9367\n",
      "Running for Batch_size = 64, Learning_rate = 0.001\n",
      "Epoch 1/1\n",
      "938/938 [==============================] - 17s 18ms/step - loss: 0.3040 - acc: : 0.9098 - validation loss: 0.0965 - validation acc: 0.9700\n",
      "Running for Batch_size = 64, Learning_rate = 0.0001\n",
      "Epoch 1/1\n",
      "938/938 [==============================] - 16s 18ms/step - loss: 1.2674 - acc: : 0.6883 - validation loss: 0.4880 - validation acc: 0.8635\n"
     ]
    }
   ],
   "source": [
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        print(f'Running for Batch_size = {batch_size}, Learning_rate = {lr}')\n",
    "        \n",
    "        # using altering batch_sizes for the train_loader\n",
    "        train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle=True)\n",
    "        \n",
    "        # initialize the network\n",
    "        model = CNN().to(device) # sending model to device\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        writer = SummaryWriter(f'runs/MNIST/MiniBatchSize {batch_size}, LR {lr}')\n",
    "        \n",
    "        # Visualize model in TensorBoard\n",
    "#         images, _ = next(iter(train_loader))\n",
    "#         writer.add_graph(model, images.to(device))\n",
    "#         writer.close()\n",
    "        \n",
    "        for epoch in range(num_epochs):  # 1 epoch means the model has seen all the images\n",
    "            losses = []\n",
    "            accuracies = []\n",
    "            \n",
    "            step = 0\n",
    "            val_step = 0\n",
    "            n_batches = len(train_loader)\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "            pbar = tf.keras.utils.Progbar(target=n_batches)\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "\n",
    "            for idx, (data, target) in enumerate(train_loader):  # enumerating to see the batch idx\n",
    "\n",
    "                # get data to cuda if possible\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                # foward\n",
    "                scores = model(data)  \n",
    "                loss = criterion(scores, target)\n",
    "                \n",
    "                writer.add_scalar('Training loss', loss, global_step = step)\n",
    "                pbar.update(idx, values=[(\"loss\",loss.item())])\n",
    "                losses.append(loss.item())\n",
    "\n",
    "                # backward\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # gradient descent step\n",
    "                optimizer.step()\n",
    "\n",
    "                # plotting the weight\n",
    "                writer.add_histogram(\"fc1\", model.fc1.weight)\n",
    "                \n",
    "                # using a running training accuracy\n",
    "                _, preds = scores.max(1)\n",
    "                num_correct = (preds == target).sum()\n",
    "                running_train_acc = float(num_correct)/float(data.shape[0])\n",
    "\n",
    "                accuracies.append(running_train_acc)\n",
    "                pbar.update(idx, values=[(\"acc: \", running_train_acc)])\n",
    "                writer.add_scalar('Training accuracy', running_train_acc, global_step = step)\n",
    "                \n",
    "                class_labels = [classes[label] for label in preds]\n",
    "                \n",
    "#                 if idx == 230:\n",
    "#                     writer.add_embedding(\n",
    "#                         features,\n",
    "#                         metadata=class_labels,\n",
    "#                         label_img=data,\n",
    "#                         global_step=batch_idx,\n",
    "#                     )\n",
    "                \n",
    "                step += 1\n",
    "                \n",
    "            # for every epoch calculate test loss\n",
    "            # validation\n",
    "            model.eval()\n",
    "\n",
    "            for idx, (data, target) in enumerate(test_loader):\n",
    "                with torch.no_grad(): # no computation for gradients\n",
    "                    # get data to cuda\n",
    "                    data = data.to(device)\n",
    "                    target = target.to(device)\n",
    "\n",
    "                    scores = model(data)\n",
    "                    loss = criterion(scores, target)\n",
    "                \n",
    "                    writer.add_scalar('Validation loss', loss, global_step = val_step)\n",
    "                    pbar.update(idx, values=[(\"validation loss\",loss.item())])\n",
    "\n",
    "                    # using a running training accuracy\n",
    "                    _, preds = scores.max(1)\n",
    "                    num_correct = (preds == target).sum()\n",
    "                    running_val_acc = float(num_correct)/float(data.shape[0])\n",
    "                    \n",
    "                    pbar.update(idx, values=[(\"validation acc\", running_val_acc)])\n",
    "                    writer.add_scalar('Validation accuracy', running_val_acc, global_step = val_step)\n",
    "                    val_step += 1\n",
    "                    \n",
    "            pbar.update(n_batches, values=None)\n",
    "            \n",
    "            # after each batch add the accuracies and losses\n",
    "            writer.add_hparams(\n",
    "                {\"lr\": lr, \"bsize\": batch_size},\n",
    "                {\n",
    "                    \"accuracy\": sum(accuracies) / len(accuracies),\n",
    "                    \"loss\": sum(losses) / len(losses),\n",
    "                },\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f560b0-f04b-432a-b449-414af980d027",
   "metadata": {},
   "source": [
    "Use the following command to view the results\n",
    "\n",
    "```\n",
    "> tensorboard --logdir runs\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
